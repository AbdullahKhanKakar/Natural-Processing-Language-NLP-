# ðŸ”¥ NLP

1. Tokenization: "Tokenization is the process of converting the text into smaller units called tokens."
   There are different ways to do it:
     - sent_tokenize
     - word_tokenize
     - wordpunct_tokenize
     - TreebankWordTokenizer
